# leaderboard/leaderboard_config.yaml

# --- Column Display Names ---
# Used to rename columns from your .jsonl files for display in the UI tables.
# Format: "Original_Column_Name_In_JSONL": "Desired Display Name in UI"
column_names:
  # Columns added by the updated refresh.py
  "Model Name": "Model" # This 'Model Name' is the canonical ID from refresh.py. Its display will be further customized by 'model_display_configs'.
  "model_url": "Link"
  "parameters_count": "Parameters"
  "source_type": "Source Type"

  # Common score columns (these are examples, use your actual metric names from .jsonl)
  # "Average": "Overall AVG"      # For the 'all' table summary
  # "strict_instruction_accuracy": "IFEval Acc."
  # "score_mean": "MT-Bench Score"
  # "acc": "Accuracy"            # Generic accuracy (e.g., for MMLU, persian_csr)
  # "nlg_score": "NLG Score"
  # "nlu_score": "NLU Score"
  # Add other specific metric columns from your .jsonl files that you want to display or rename
  # e.g., "exact_match": "Exact Match", "f1": "F1 Score"

# --- Task (Tab) Display Names & Identifiers ---
# Defines the display names for UI tabs and identifies tasks for processing.
# Keys MUST match the base filenames of .jsonl files produced by refresh.py (e.g., "MMLU" for "MMLU.jsonl").
# These keys are also used in 'main_scores_map' and 'tab_processing_order'.
task_display_names:
  all: "üèÜ Overall Benchmark"
  mt_bench: "Persian MT-Bench"
  ifeval: "IFEval-Fa"
  MMLU: "MMLU-Fa"
  persian_csr: "PerCoR"
  persian_nlg: "Persian NLG" # Overview tab
  persian_nlu: "Persian NLU"



# --- Model Display Configurations ---
# Customize how model names are displayed and potentially override their URLs.
# Keys here MUST be the canonical model identifiers found in the "Model Name" column
# (or whatever column is specified by 'model_identifier_column' in global_settings)
# of the .jsonl files generated by refresh.py.
model_display_configs:
  # Replace these with your actual model identifiers and desired display names/URLs
  "example-org/example-model-v1": # This is an example of an internal/canonical model ID
    display_name: "Example Model v1 (Custom Name)"
    # url: "https://example.com/my-model-v1" # Optional: Overrides URL from data. If absent, URL from data is used.
  
  "gpt-04-internal-id": # Replace with the actual canonical ID refresh.py outputs for GPT-4
    display_name: "GPTO4"
    # url: "https://openai.com/gpt-4" # Example override

  "deepseek-chat":
    display_name: "DeepSeek-R1"
    url: 'www.google.com'

  # Add one entry for each model whose display name or URL you want to customize.
  # If a model ID from your data is not listed here, its raw ID will be used as its name.

# --- Global Settings ---
# Various settings controlling leaderboard behavior and data interpretation.
global_settings:
  # The actual column name in your .jsonl DataFrames that holds the canonical model identifier.
  # This identifier is used as the key to look up entries in 'model_display_configs'.
  model_identifier_column: "Model Name"

  # Defines the primary score column used for ranking within each task's table.
  # Keys MUST match the keys in 'task_display_names' and 'tab_processing_order'.
  # Values MUST be actual column names present in the corresponding .jsonl data files.
  main_scores_map:
    all: "Average"
    mt_bench: "score_mean"
    ifeval: "strict_instruction_accuracy"
    MMLU: "acc"
    persian_csr: "acc"
    persian_nlg: "nlg_score"
    persian_nlu: "nlu_score"
    question-generation_PersianQA: "nlg_score"
    translation-en2fa_en2fa: "nlg_score"
    translation-fa2en_fa2en: "nlg_score"
    translation-ar2fa_ar2fa: "nlg_score"
    translation-fa2ar_fa2ar: "nlg_score"
    summarization_SamSUM-fa: "nlg_score"
    summarization_PnSummary: "nlg_score"

    sentiment-analysis_deepsentipers: "nlu_score"
    sts_SynPerSTS: "nlu_score"
    ner_arman: "nlu_score"
    keyword-extraction_SynKeywords: "nlu_score"
    tone-classification_SynTone: "nlu_score"
    sts_FarSICK: "nlu_score"
    paraphrase-detection_FarsiParaphraseDetection: "nlu_score"
    nli_farstail: "nlu_score"
    paraphrase-detection_parsinlu: "nlu_score"
    extractive-qa_PQuAD: "nlu_score"
    topic-classification_sid: "nlu_score"



  # Original column names in DataFrames that are allowed to have null values if an 'Average'
  # score is calculated by leaderboard.py (though refresh.py now handles the 'all' table Average).
  # Primarily non-score informational columns.
  allowed_null_columns_in_average:
    - "Model Name"
    - "model_url"
    - "parameters_count"
    - "source_type"

  # A score threshold, its specific use in formatting needs to be defined in leaderboard.py if used.
  # For example, conditional formatting for scores above this value. (Currently not actively used in provided Python code for formatting).
  score_cutoff_for_formatting: 0.0

  # Defines the order of tabs in the UI and the order for loading data files by leaderboard.py.
  # Keys MUST match:
  #   1. Keys in 'task_display_names'.
  #   2. Keys in 'main_scores_map'.
  #   3. The base names of .jsonl files generated by refresh.py (e.g., "MMLU" for "MMLU.jsonl").
  tab_processing_order:
    - "all"
    - "mt_bench"
    - "ifeval"
    - "MMLU"
    - "persian_csr"
    - "persian_nlg"
    - "persian_nlu"
  
  numeric_score_columns_for_bolding: # List of ORIGINAL column names
    # For the "Overall Benchmark" tab (all.jsonl)
    - "Average"
    - "IFEval-Fa"
    - "Persian MT-Bench"
    - "MMLU-Fa"
    - "PerCoR"
    - "Persian NLU"
    - "Persian NLG"
    
    # For individual task tabs (if you want to keep their main scores bolded there)
    # These are typically the values from your 'main_scores_map'
    - "score_mean"                  # For mt_bench tab
    - "strict_instruction_accuracy" # For ifeval tab
    - "acc"                         # For MMLU, persian_csr tabs
    # "nlg_score" and "nlu_score" are already covered if "Persian NLG" 
    # and "Persian NLU" are the actual column names in those specific tabs too.
    # If persian_nlg.jsonl uses "nlg_score" as its main column, and 
    # persian_nlu.jsonl uses "nlu_score", then you can add them for those specific tabs:
    - "nlg_score"                   # For persian_nlg tab (if it's different from "Persian NLG")
    - "nlu_score"                   # For persian_nlu tab (if it's different from "Persian NLU")
    # Add any other specific metric columns from other .jsonl files 
    # that you want to have their max value bolded in their respective tabs.

  # Add this list:
  columns_to_hide: # List of ORIGINAL column names you don't want to display
    - "model_url"
    - "source_type"
    # - "another_column_to_hide" 

  parent_child_task_map:
    persian_nlg: # Parent task key
      - "question-generation_PersianQA"
      - "translation-en2fa_en2fa"
      - "translation-fa2en_fa2en"
      - "translation-ar2fa_ar2fa"
      - "translation-fa2ar_fa2ar"
      - "summarization_SamSUM-fa"
      - "summarization_PnSummary"
    persian_nlu: # Parent task key
      - "sentiment-analysis_deepsentipers"
      - "sts_SynPerSTS"
      - "ner_arman"
      - "keyword-extraction_SynKeywords"
      - "tone-classification_SynTone"
      - "sts_FarSICK"
      - "paraphrase-detection_FarsiParaphraseDetection"
      - "nli_farstail"
      - "paraphrase-detection_parsinlu" # Assumin g this was a typo or duplicate key
      - "extractive-qa_PQuAD"
      - "topic-classification_sid"
