# leaderboard/leaderboard_config.yaml

# --- Column Display Names ---
# Used to rename columns from your .jsonl files for display in the UI tables.
# Format: "Original_Column_Name_In_JSONL": "Desired Display Name in UI"
column_names:
  # Columns added by the updated refresh.py
  "Model Name": "Model" # This 'Model Name' is the canonical ID from refresh.py. Its display will be further customized by 'model_display_configs'.
  "model_url": "Link"
  "parameters_count": "Parameters"
  "source_type": "Source Type"
  "Average": "Average"
  "score_mean": "score_mean (main)"
  "strict_instruction_accuracy": "strict_instruction_accuracy (main)"
  "acc": "accuracy (main)"
  "nlg_score": "nlg_score (main)"
  "nlu_score": "nlu_score (main)"

  # Common score columns (these are examples, use your actual metric names from .jsonl)
  # "Average": "Overall AVG"      # For the 'all' table summary
  # "strict_instruction_accuracy": "IFEval Acc."
  # "score_mean": "MT-Bench Score"
  # "acc": "Accuracy"            # Generic accuracy (e.g., for MMLU, persian_csr)
  # "nlg_score": "NLG Score"
  # "nlu_score": "NLU Score"
  # Add other specific metric columns from your .jsonl files that you want to display or rename
  # e.g., "exact_match": "Exact Match", "f1": "F1 Score"

# --- Task (Tab) Display Names & Identifiers ---
# Defines the display names for UI tabs and identifies tasks for processing.
# Keys MUST match the base filenames of .jsonl files produced by refresh.py (e.g., "MMLU" for "MMLU.jsonl").
# These keys are also used in 'main_scores_map' and 'tab_processing_order'.
task_display_names:
  all: "üèÜ Overall Benchmark"
  mt_bench: "Persian MT-Bench"
  ifeval: "Persian IFEval"
  MMLU: "PerMMLU"
  persian_csr: "PerCoR"
  persian_nlg: "Persian NLG" # Overview tab
  persian_nlu: "Persian NLU"
  question-generation_PersianQA: "PersianQA (QG)"
  translation-en2fa_en2fa: "Translation (en2fa)"
  translation-fa2en_fa2en: "Translation (fa2en)"
  translation-ar2fa_ar2fa: "Translation (ar2fa)"
  translation-fa2ar_fa2ar: "Translation (fa2ar)"
  summarization_SamSUM-fa: "SamSum-Fa (Summarizaion)"
  summarization_PnSummary: "PnSummary (Summarizaion)"
  sentiment-analysis_deepsentipers: "DeepSentiPers (SA)"
  sts_SynPerSTS: "SynPerSTS (STS)"
  ner_arman: "Arman (NER)"
  keyword-extraction_SynKeywords: "SynKeywords (Keyword Extraction)"
  tone-classification_SynTone: "SynTone (Tone Classification)"
  sts_FarSICK: "FarSICK (STS)"
  paraphrase-detection_FarsiParaphraseDetection: "FarsiParaphraseDetection (Paraphrase Detection)"
  nli_farstail: "Farstail (NLI)"
  paraphrase-detection_parsinlu: "ParsiNLU (Paraphrase Detection)" # Assuming this was a typo or duplicate key and intended for update
  extractive-qa_PQuAD: "PQuAD (Extractive QA)"
  topic-classification_sid: "SID (Topic Classification)"



# --- Model Display Configurations ---
# Customize how model names are displayed and potentially override their URLs.
# Keys here MUST be the canonical model identifiers found in the "Model Name" column
# (or whatever column is specified by 'model_identifier_column' in global_settings)
# of the .jsonl files generated by refresh.py.
model_display_configs:
  "claude-3-7-sonnet-20250219":
    display_name: "Claude 3.7 Sonnet"
    url: "https://www.anthropic.com/news/claude-3-7-sonnet"

  "gpt-4.1":
    display_name: "GPT-4.1"
    url: "https://openai.com/index/gpt-4-1/"

  "gpt-4o":
    display_name: "GPT-4o"
    url: "https://openai.com/index/hello-gpt-4o/"

  "gpt-4.1-mini":
    display_name: "GPT-4.1 Mini"
    url: "https://openai.com/index/gpt-4-1/"

  "deepseek-chat":
    display_name: "DeepSeek-V3"
    url: "https://api-docs.deepseek.com/"

  "gemma-3-27b-it":
    display_name: "Gemma 3 27B IT"
    url: "https://huggingface.co/google/gemma-3-27b-it"

  "gpt-4o-mini":
    display_name: "GPT-4o Mini"
    url: "https://openai.com/index/hello-gpt-4o/"

  "Qwen3-32B":
    display_name: "Qwen3-32B"
    url: "https://huggingface.co/Qwen/Qwen3-32B"

  "Llama-3.3-70B-Instruct":
    display_name: "Llama 3.3 70B Instruct"
    url: "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"

  "gemma-3-12b-it":
    display_name: "Gemma 3 12B IT"
    url: "https://huggingface.co/google/gemma-3-12b-it"

  "Qwen3-14B":
    display_name: "Qwen3-14B"
    url: "https://huggingface.co/Qwen/Qwen3-14B"

  "Mistral-Small-3.1-24B-Instruct-2503":
    display_name: "Mistral Small 3.1 24B Instruct"
    url: "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"

  "claude-3-5-haiku-20241022":
    display_name: "Claude 3.5 Haiku"
    url: "https://www.anthropic.com/claude/haiku"

  "gpt-4.1-nano":
    display_name: "GPT-4.1 Nano"
    url: "https://openai.com/index/gpt-4-1/"

  "Qwen3-8B":
    display_name: "Qwen3-8B"
    url: "https://huggingface.co/Qwen/Qwen3-8B"

  "gemma-3-4b-it":
    display_name: "Gemma 3 4B IT"
    url: "https://huggingface.co/google/gemma-3-4b-it"

  "aya-expanse-32b":
    display_name: "Aya Expanse 32B"
    url: "https://huggingface.co/CohereLabs/aya-expanse-32b"

  "Qwen3-4B":
    display_name: "Qwen3-4B"
    url: "https://huggingface.co/Qwen/Qwen3-4B"

  "gemma-3-1b-it":
    display_name: "Gemma 3 1B IT"
    url: "https://huggingface.co/google/gemma-3-1b-it"

  "Mistral-7B-Instruct-v0.3":
    display_name: "Mistral 7B Instruct v0.3"
    url: "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"

  "Llama-3.2-3B-Instruct":
    display_name: "Llama 3.2 3B Instruct"
    url: "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"

  "Llama-3.2-1B-Instruct":
    display_name: "Llama 3.2 1B Instruct"
    url: "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"

  "o4-mini":
    display_name: "GPT-o4 Mini"
    url: "https://openai.com/index/introducing-o3-and-o4-mini/"

  "deepseek-reasoner":
    display_name: "DeepSeek-R1"
    url: "https://api-docs.deepseek.com/guides/reasoning_model"

  "gemini-2.0-flash":
    display_name: "Gemini 2.0 Flash"
    url: "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash"

  "gemini-2.5-flash-preview-05-20":
    display_name: "Gemini 2.5 Flash Preview"
    url: "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash"

  "Qwen3-30B-A3B":
    display_name: "Qwen3-30B-A3B"
    url: "https://huggingface.co/Qwen/Qwen3-30B-A3B"

  "c4ai-command-r-plus":
    display_name: "Command R Plus"
    url: "https://huggingface.co/CohereLabs/c4ai-command-r-plus"

  "c4ai-command-r-v01":
    display_name: "Command R v01"
    url: "https://huggingface.co/CohereLabs/c4ai-command-r-v01"

  "c4ai-command-a-03-2025":
    display_name: "Command A"
    url: "https://huggingface.co/CohereLabs/c4ai-command-a-03-2025"



  # Add one entry for each model whose display name or URL you want to customize.
  # If a model ID from your data is not listed here, its raw ID will be used as its name.

# --- Global Settings ---
# Various settings controlling leaderboard behavior and data interpretation.
global_settings:
  # The actual column name in your .jsonl DataFrames that holds the canonical model identifier.
  # This identifier is used as the key to look up entries in 'model_display_configs'.
  model_identifier_column: "Model Name"

  # Defines the primary score column used for ranking within each task's table.
  # Keys MUST match the keys in 'task_display_names' and 'tab_processing_order'.
  # Values MUST be actual column names present in the corresponding .jsonl data files.
  main_scores_map:
    all: "Average"
    mt_bench: "score_mean"
    ifeval: "strict_instruction_accuracy"
    MMLU: "acc"
    persian_csr: "acc"
    persian_nlg: "nlg_score"
    persian_nlu: "nlu_score"
    question-generation_PersianQA: "nlg_score"
    translation-en2fa_en2fa: "nlg_score"
    translation-fa2en_fa2en: "nlg_score"
    translation-ar2fa_ar2fa: "nlg_score"
    translation-fa2ar_fa2ar: "nlg_score"
    summarization_SamSUM-fa: "nlg_score"
    summarization_PnSummary: "nlg_score"

    sentiment-analysis_deepsentipers: "nlu_score"
    sts_SynPerSTS: "nlu_score"
    ner_arman: "nlu_score"
    keyword-extraction_SynKeywords: "nlu_score"
    tone-classification_SynTone: "nlu_score"
    sts_FarSICK: "nlu_score"
    paraphrase-detection_FarsiParaphraseDetection: "nlu_score"
    nli_farstail: "nlu_score"
    paraphrase-detection_parsinlu: "nlu_score"
    extractive-qa_PQuAD: "nlu_score"
    topic-classification_sid: "nlu_score"



  # Original column names in DataFrames that are allowed to have null values if an 'Average'
  # score is calculated by leaderboard.py (though refresh.py now handles the 'all' table Average).
  # Primarily non-score informational columns.
  allowed_null_columns_in_average:
    - "Model Name"
    - "model_url"
    - "parameters_count"
    - "source_type"

  # A score threshold, its specific use in formatting needs to be defined in leaderboard.py if used.
  # For example, conditional formatting for scores above this value. (Currently not actively used in provided Python code for formatting).
  score_cutoff_for_formatting: 0.0

  # Defines the order of tabs in the UI and the order for loading data files by leaderboard.py.
  # Keys MUST match:
  #   1. Keys in 'task_display_names'.
  #   2. Keys in 'main_scores_map'.
  #   3. The base names of .jsonl files generated by refresh.py (e.g., "MMLU" for "MMLU.jsonl").
  tab_processing_order:
    - "all"
    - "mt_bench"
    - "ifeval"
    - "MMLU"
    - "persian_csr"
    - "persian_nlg"
    - "persian_nlu"
  
  numeric_score_columns_for_bolding: # List of ORIGINAL column names
    # For the "Overall Benchmark" tab (all.jsonl)
    - "Average"
    - "Persian IFEval"
    - "Persian MT-Bench"
    - "PerMMLU"
    - "PerCoR"
    - "Persian NLU"
    - "Persian NLG"
    
    # For individual task tabs (if you want to keep their main scores bolded there)
    # These are typically the values from your 'main_scores_map'
    - "score_mean"                  # For mt_bench tab
    - "strict_instruction_accuracy" # For ifeval tab
    - "acc"                         # For MMLU, persian_csr tabs
    # "nlg_score" and "nlu_score" are already covered if "Persian NLG" 
    # and "Persian NLU" are the actual column names in those specific tabs too.
    # If persian_nlg.jsonl uses "nlg_score" as its main column, and 
    # persian_nlu.jsonl uses "nlu_score", then you can add them for those specific tabs:
    - "nlg_score"                   # For persian_nlg tab (if it's different from "Persian NLG")
    - "nlu_score"                   # For persian_nlu tab (if it's different from "Persian NLU")
    # Add any other specific metric columns from other .jsonl files 
    # that you want to have their max value bolded in their respective tabs.

  # Add this list:
  columns_to_hide: # List of ORIGINAL column names you don't want to display
    - "model_url"
    - "source_type"
    # - "another_column_to_hide" 

  parent_child_task_map:
    persian_nlg: # Parent task key
      - "question-generation_PersianQA"
      - "translation-en2fa_en2fa"
      - "translation-fa2en_fa2en"
      - "translation-ar2fa_ar2fa"
      - "translation-fa2ar_fa2ar"
      - "summarization_SamSUM-fa"
      - "summarization_PnSummary"
    persian_nlu: # Parent task key
      - "sentiment-analysis_deepsentipers"
      - "sts_SynPerSTS"
      - "ner_arman"
      - "keyword-extraction_SynKeywords"
      - "tone-classification_SynTone"
      - "sts_FarSICK"
      - "paraphrase-detection_FarsiParaphraseDetection"
      - "nli_farstail"
      - "paraphrase-detection_parsinlu" # Assumin g this was a typo or duplicate key
      - "extractive-qa_PQuAD"
      - "topic-classification_sid"
